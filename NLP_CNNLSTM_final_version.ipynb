{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKUNhsxcxLBc"
   },
   "source": [
    "#Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JqVLyEiUaTL"
   },
   "outputs": [],
   "source": [
    "# useful libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer       \n",
    "from sklearn.utils import shuffle   \n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4lNndadJOvV",
    "outputId": "eaf836fe-4e48-4aa1-aa8e-2ecd4ee8e80b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxMhT4iZJYlU"
   },
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQcBwVpWKs_D"
   },
   "source": [
    "# Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67IvLcK7LGfj",
    "outputId": "69f5c201-3733-4ec2-bc1b-2745b9757b8e"
   },
   "outputs": [],
   "source": [
    "!pip3 install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MPvEMbdKvNU"
   },
   "outputs": [],
   "source": [
    "path='./gdrive/MyDrive/nlp_data/'\n",
    "\n",
    "emb_size=302\n",
    "import pickle5 as pickle\n",
    "with open(path+'emb_by_essay_id.pickle', 'rb') as f:\n",
    "    data = pickle.load(f) # its a dict , every entry is one essay and words are 302 size embeddings, last two are categories and indicators is the word mispelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQbJ3Dm0uJy5"
   },
   "outputs": [],
   "source": [
    "path='./gdrive/MyDrive/nlp_data/'\n",
    "\n",
    "emb_size=301\n",
    "import pickle5 as pickle\n",
    "with open(path+'emb_by_essay_id_no_correction.pickle', 'rb') as f:\n",
    "    data = pickle.load(f) # its a dict , every entry is one essay and words are 302 size embeddings, last two are categories and indicators is the word mispelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "MfDcddAcLkoU",
    "outputId": "d67459f7-c7a1-4233-9e14-198782939dd9"
   },
   "outputs": [],
   "source": [
    "# Put data in a list\n",
    "mini=10000\n",
    "maxi=0\n",
    "lens=[]\n",
    "data_list=[]\n",
    "for i in data.keys():\n",
    "    if mini>len(data[i]):\n",
    "      mini=len(data[i])\n",
    "    if maxi<len(data[i]):\n",
    "      maxi=len(data[i])\n",
    "\n",
    "    lens.append(len(data[i]))\n",
    "    for j in range(len(data[i])):\n",
    "      data[i][j]=data[i][j].reshape(1,-1)\n",
    "\n",
    "\n",
    "for i in data.keys():\n",
    "   data_list.append(np.concatenate(data[i],axis=0)) # every data point/essay is a word_num x 302 matrix\n",
    "\n",
    "print(mini)\n",
    "print(maxi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOP5evaNjCUJ",
    "outputId": "c2174d99-1b9b-4553-8bf6-8355e4cf3206"
   },
   "outputs": [],
   "source": [
    "# DO train/test split with chosen indexes and chosen label\n",
    "chosen_score=1\n",
    "\n",
    "with open('scores.pickle', 'rb') as f:\n",
    "    scores = pickle.load(f)\n",
    "\n",
    "with open('ids_for_test.pickle', 'rb') as f:\n",
    "    ids_essays_test = pickle.load(f)\n",
    "\n",
    "ids_all=np.asarray(list(data.keys()))\n",
    "print(ids_all.shape)\n",
    "x_test=[]\n",
    "scores_test=[]\n",
    "x_train=[]\n",
    "scores_train=[]\n",
    "y_test_reg=[]\n",
    "y_train_reg=[]\n",
    "\n",
    "\n",
    "for i in range(len(ids_all)):\n",
    "    if ids_all[i] in ids_essays_test[chosen_score]:\n",
    "        x_test.append(data_list[i])\n",
    "        scores_test.append(scores[i,chosen_score])\n",
    "        y_test_reg.append(np.sum(scores[i,:]))\n",
    "    else:\n",
    "        x_train.append(data_list[i])\n",
    "        scores_train.append(scores[i,chosen_score])\n",
    "        y_train_reg.append(np.sum(scores[i,:]))\n",
    "\n",
    "y_train=np.asarray(scores_train)\n",
    "y_test=np.asarray(scores_test)\n",
    "\n",
    "y_train_reg=np.asarray(y_train_reg).astype('float')\n",
    "y_test_reg=np.asarray(y_test_reg).astype('float')\n",
    "\n",
    "print(y_train.shape)\n",
    "print(len(x_train))\n",
    "print(y_test.shape)\n",
    "print(len(x_test))\n",
    "print(y_train_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUusUOnYJOES"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrXeiPwvMZNV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPLwfY1pxHNA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from torch.utils.data import Sampler\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def collate(examples):\n",
    "    \n",
    "    seq = [torch.tensor(e[0]) for e in examples]\n",
    "    lens = np.asarray([e[1] for e in examples])\n",
    "    labels = np.asarray([e[2] for e in examples])\n",
    "    return pad_sequence(seq, batch_first=True, padding_value=0), lens , labels\n",
    "\n",
    "class BySequenceLengthSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source,  \n",
    "                bucket_boundaries, batch_size=64, drop_last=True):\n",
    "        \n",
    "        ind_n_len = []\n",
    "        for i, p in enumerate(data_source): # p is the data\n",
    "            ind_n_len.append( (i, p[1]) ) # has indexes and lens of all data\n",
    "        data=[]\n",
    "        for i, p in enumerate(data_source): # p is the data\n",
    "            data.append( (p[0], p[2]) ) #(sequence, label)\n",
    "\n",
    "        self.data_source = data\n",
    "        self.ind_n_len = ind_n_len\n",
    "        self.bucket_boundaries = bucket_boundaries\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if self.drop_last:\n",
    "            print(\"WARNING: drop_last=True, dropping last non batch-size batch in every bucket ... \")\n",
    "\n",
    "        self.boundaries = list(self.bucket_boundaries)\n",
    "        self.buckets_min = torch.tensor([np.iinfo(np.int32).min] + self.boundaries)\n",
    "        self.buckets_max = torch.tensor(self.boundaries + [np.iinfo(np.int32).max])\n",
    "        self.boundaries = torch.tensor(self.boundaries)\n",
    "\n",
    "    def shuffle_tensor(self, t):\n",
    "        return t[torch.randperm(len(t))]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        data_buckets = dict()\n",
    "        # where p is the id number and seq_len is the length of this id number. \n",
    "        for p, seq_len in self.ind_n_len:\n",
    "            pid = self.element_to_bucket_id(p,seq_len) # find in which bucket to put new data\n",
    "            if pid in data_buckets.keys(): # if that bucket exists add to it other make new one\n",
    "                data_buckets[pid].append(p)\n",
    "            else:\n",
    "                data_buckets[pid] = [p] # buckets have data indexes\n",
    "\n",
    "        for k in data_buckets.keys(): # every bucket to tensor\n",
    "            data_buckets[k] = torch.tensor(data_buckets[k])\n",
    "\n",
    "        iter_list = []\n",
    "        for k in data_buckets.keys():\n",
    "\n",
    "            t = self.shuffle_tensor(data_buckets[k]) #shuffle data inside every bucket\n",
    "            batch = torch.split(t, self.batch_size, dim=0) # split bucket on batches\n",
    "\n",
    "            if self.drop_last and len(batch[-1]) != self.batch_size:\n",
    "                batch = batch[:-1]\n",
    "\n",
    "            iter_list += batch\n",
    "\n",
    "        shuffle(iter_list) # shuffle all the batches so they arent ordered by bucket\n",
    "        # size\n",
    "        for i in iter_list:  # generator that returns batches of data indexes that are in same bucket\n",
    "            yield i.numpy().tolist() # as it was stored in an array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    \n",
    "    def element_to_bucket_id(self, x, seq_length):\n",
    "\n",
    "        valid_buckets = (seq_length >= self.buckets_min)*(seq_length < self.buckets_max)\n",
    "        bucket_id = valid_buckets.nonzero()[0].item()\n",
    "\n",
    "        return bucket_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJprsXmhqq3-"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKa1dqgZqqM4"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, device,hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        ## add this model the same same device with the RNN\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, rnn_outputs, final_hidden_state):\n",
    "      attn_weights = self.attn(rnn_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "      attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))\n",
    "      attn_weights = F.softmax(attn_weights.squeeze(2), dim=1)\n",
    "      context = torch.bmm(rnn_outputs.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\n",
    "      attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))\n",
    "      return attn_hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j22OibDo7cF"
   },
   "source": [
    "## CNN-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kVOXLxXVkx0"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, lens, y):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert len(x) == y.shape[0] # assuming shape[0] = dataset size\n",
    "        assert len(x) == lens.shape[0]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.lens = lens\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.lens[index], self.y[index]\n",
    "\n",
    "\n",
    "def masked_mean(tensor, lens, axis):\n",
    "\n",
    "    index = torch.tensor(lens, dtype = torch.float).to(device)\n",
    "    help=(torch.arange(tensor.shape[1])[None, :]).to(device)\n",
    "    mask = ((help < index[:, None])*1).to(device)\n",
    "    mask = torch.unsqueeze(mask,2)\n",
    "\n",
    "    masked = torch.mul(tensor, mask)  # Apply the mask using an element-wise multiply\n",
    "    return masked.sum(axis) / mask.sum(axis)  # Find the average!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgMl2uKmO6O7"
   },
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, num_class):\n",
    "        super().__init__()\n",
    "   \n",
    "        self.cnn= nn.Conv1d(in_channels=input_size, out_channels=150, kernel_size=3, stride=1, padding=1, padding_mode='zeros') #same padding.out will be batch_size, outchannel, same length \n",
    "        self.cnn2= nn.Conv1d(in_channels=75, out_channels=150, kernel_size=3, stride=1, padding=1, padding_mode='zeros')\n",
    "        \n",
    "        self.cnn1= nn.Conv1d(in_channels=input_size, out_channels=150, kernel_size=5, stride=1, padding=2, padding_mode='zeros') #same padding.out will be batch_size, outchannel, same length     \n",
    "        self.cnn3= nn.Conv1d(in_channels=75, out_channels=150, kernel_size=5, stride=1, padding=2, padding_mode='zeros')\n",
    "\n",
    "\n",
    "        self.max_pool1= nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size = 64, batch_first = True, num_layers=2, dropout = 0.4, bidirectional=True) #300 before in ch1\n",
    "        \n",
    "        self.linear1 = nn.Linear(128, 200)\n",
    "        self.linear2 = nn.Linear(200, 100)\n",
    "\n",
    "       \n",
    "        self.linear_end = nn.Linear(100, num_class)\n",
    "      \n",
    "\n",
    "\n",
    "    def forward(self, input_seq, lens):\n",
    "    #\n",
    "       # print(f\"Input shape {input_seq.shape}\")\n",
    "\n",
    "       \n",
    "\n",
    "        cnn_out = F.leaky_relu( self.cnn(input_seq))\n",
    "       # print(f\"CNN_out1 shape {cnn_out.shape}\")\n",
    "\n",
    "        pool1=self.max_pool1(cnn_out.transpose(1,2)).transpose(1,2)\n",
    "      #  print(f\"Pool_out1 shape {pool1.shape}\")\n",
    "\n",
    "        cnn_out2 = F.leaky_relu( self.cnn2(pool1)).transpose(1,2) \n",
    "       # print(f\"CNN_out11 shape {cnn_out2.shape}\")\n",
    "\n",
    "        cnn_out1 = F.leaky_relu(self.cnn1(input_seq))\n",
    "       # print(f\"CNN_out2 shape {cnn_out1.shape}\")\n",
    "\n",
    "        pool2=self.max_pool1(cnn_out1.transpose(1,2)).transpose(1,2)\n",
    "        #print(f\"Pool_out1 shape {pool2.shape}\")\n",
    "\n",
    "        cnn_out3 = F.leaky_relu(self.cnn3(pool2)).transpose(1,2)\n",
    "       # print(f\"CNN_out21 shape {cnn_out3.shape}\")\n",
    "\n",
    "        cnn_concat = torch.cat((cnn_out2, cnn_out3), 2)\n",
    "       # print(f\"CNN_concat shape {cnn_concat.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        packed_seq = pack_padded_sequence(cnn_concat, lens, batch_first = True, enforce_sorted= False) # pack for lstm for masking\n",
    "       # print(f\"packed out shape {packed_seq.data.shape}\")\n",
    "        lstm_out, self.hidden_cell = self.lstm(packed_seq)\n",
    "  \n",
    "\n",
    "        result_unpacked, lengths_unpacked = pad_packed_sequence(lstm_out, batch_first = True)\n",
    "       # print(f\"LSTM out shape {result_unpacked.shape}\")\n",
    "  \n",
    "      \n",
    "        mean_over_time= masked_mean(result_unpacked, lens, 1)\n",
    "      #  print(f\"Mean out shape {mean_over_time.shape}\")\n",
    "\n",
    "    \n",
    "\n",
    "        lin_out1 =  F.relu(self.linear1(mean_over_time))\n",
    "       #print(f\"Linear1 out shape {lin_out1.shape}\")\n",
    "        lin_out2 = F.relu(self.linear2(lin_out1))\n",
    "       # print(f\"Linear2 out shape {lin_out2.shape}\")\n",
    "\n",
    "        predictions = self.linear_end(lin_out2)\n",
    "       \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kGpaYlmrDAu"
   },
   "source": [
    "## A-CNN-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqVREEsGrFd3"
   },
   "outputs": [],
   "source": [
    "class ACNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, num_class):\n",
    "        super().__init__()\n",
    "     \n",
    "        self.cnn= nn.Conv1d(in_channels=input_size, out_channels=150, kernel_size=3, stride=1, padding=1, padding_mode='zeros') #same padding.out will be batch_size, outchannel, same length \n",
    "        self.cnn2= nn.Conv1d(in_channels=75, out_channels=150, kernel_size=3, stride=1, padding=1, padding_mode='zeros')\n",
    "        \n",
    "        self.cnn1= nn.Conv1d(in_channels=input_size, out_channels=150, kernel_size=5, stride=1, padding=2, padding_mode='zeros') #same padding.out will be batch_size, outchannel, same length     \n",
    "        self.cnn3= nn.Conv1d(in_channels=75, out_channels=150, kernel_size=5, stride=1, padding=2, padding_mode='zeros')\n",
    "\n",
    "\n",
    "        self.max_pool1= nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size = 64, batch_first = True, num_layers=2, dropout = 0.4, bidirectional=True) #300 before in ch1\n",
    "        \n",
    "        self.linear1 = nn.Linear(128, 200)\n",
    "        self.linear2 = nn.Linear(200, 100)\n",
    "\n",
    "\n",
    "        # define an attention layer\n",
    "        self.attention = Attention(device,128)\n",
    "\n",
    "       \n",
    "        self.linear_end = nn.Linear(100, num_class)\n",
    "      \n",
    "\n",
    "\n",
    "    def forward(self, input_seq, lens):\n",
    "    \n",
    "       # print(f\"Input shape {input_seq.shape}\")\n",
    "\n",
    "        cnn_out = F.leaky_relu( self.cnn(input_seq))\n",
    "       #print(f\"CNN_out1 shape {cnn_out.shape}\")\n",
    "\n",
    "        pool1=self.max_pool1(cnn_out.transpose(1,2)).transpose(1,2)\n",
    "       #print(f\"Pool_out1 shape {pool1.shape}\")\n",
    "\n",
    "        cnn_out2 = F.leaky_relu( self.cnn2(pool1)).transpose(1,2) \n",
    "       # print(f\"CNN_out11 shape {cnn_out2.shape}\")\n",
    "\n",
    "        cnn_out1 = F.leaky_relu(self.cnn1(input_seq))\n",
    "       # print(f\"CNN_out2 shape {cnn_out1.shape}\")\n",
    "\n",
    "        pool2=self.max_pool1(cnn_out1.transpose(1,2)).transpose(1,2)\n",
    "        #print(f\"Pool_out1 shape {pool2.shape}\")\n",
    "\n",
    "        cnn_out3 = F.leaky_relu(self.cnn3(pool2)).transpose(1,2) \n",
    "       #print(f\"CNN_out21 shape {cnn_out3.shape}\")\n",
    "\n",
    "        cnn_concat = torch.cat((cnn_out2, cnn_out3), 2)\n",
    "       # print(f\"CNN_concat shape {cnn_concat.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "        packed_seq = pack_padded_sequence(cnn_concat, lens, batch_first = True, enforce_sorted= False) # pack for lstm for masking\n",
    "      \n",
    "        lstm_out, self.hidden_cell = self.lstm(packed_seq)\n",
    "    \n",
    "\n",
    "        result_unpacked, lengths_unpacked = pad_packed_sequence(lstm_out, batch_first = True)\n",
    "       # print(f\"LSTM out shape {result_unpacked.shape}\")\n",
    "   \n",
    "        ## Collect last hidden state\n",
    "        final_state = self.hidden_cell[0].view(2, 2, result_unpacked.shape[0], 64)[-1]\n",
    "        # Since it is bidirectional\n",
    "        h_1, h_2 = final_state[0], final_state[1]\n",
    "        final_hidden_state = torch.cat((h_1, h_2), 1)  # Concatenate both states\n",
    "\n",
    "        # Attention layer\n",
    "        att_out, attention_weights = self.attention(result_unpacked, final_hidden_state)\n",
    "       # print(f\"Attention out shape {att_out.shape}\")\n",
    "\n",
    "    \n",
    "        lin_out1 =  F.relu(self.linear1(att_out))\n",
    "        #print(f\"Linear1 out shape {lin_out1.shape}\")\n",
    "        lin_out2 = F.relu(self.linear2(lin_out1))\n",
    "       # print(f\"Linear2 out shape {lin_out2.shape}\")\n",
    "\n",
    "        predictions = self.linear_end(lin_out2)\n",
    "       \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiBLZ6vaJQUm"
   },
   "source": [
    "# Debugg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlHWxcY5qx6x",
    "outputId": "70bc2dcc-09c8-4601-8f85-5002196411e8"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_mat = np.ones((4,3,302))\n",
    "lengths = np.ones(4)\n",
    "lengths[0]=3\n",
    "lengths[1]=3\n",
    "lengths[2]=1\n",
    "lengths[3]=2\n",
    "input_tens = torch.tensor(input_mat, dtype = torch.float).transpose(1,2).to(device) # vector x len_essay\n",
    "model = ACNNLSTM(302,4)\n",
    "model.to(device)\n",
    "result = model(input_tens,lengths)\n",
    "print(result.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wXN1Vojoabg"
   },
   "source": [
    "# Split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOExd5gKXtYe"
   },
   "outputs": [],
   "source": [
    "# Random split to get indexes of test set 10% for diff scores\n",
    "\n",
    "def train_test_split(labels, test_perc=0.1):# train is list of essays that are len x 302 arrays\n",
    "    \n",
    "     all_labels=set(labels)\n",
    "    \n",
    "     test_ind=[]\n",
    "     train_ind=[]\n",
    "     class_lens_train=[]\n",
    "     for l in all_labels:\n",
    "         \n",
    "         Idx=np.where(labels==l)[0]\n",
    "         \n",
    "         \n",
    "         test_size=int(len(Idx)*test_perc)\n",
    "\n",
    "         np.random.shuffle(Idx) # shuffle labels\n",
    "    \n",
    "         idx=Idx[:test_size]\n",
    "         idx1=Idx[test_size:]\n",
    "         test_ind.append(idx)\n",
    "         train_ind.append(idx1)\n",
    "         \n",
    "\n",
    "         class_lens_train.append(len(idx1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     test_ind=np.concatenate(test_ind)\n",
    "     train_ind=np.concatenate(train_ind)\n",
    "     class_lens_train=np.asarray(class_lens_train)\n",
    "\n",
    "     return  train_ind, test_ind, class_lens_train, len(list(all_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aBsqLky0qVF"
   },
   "source": [
    "# Training full batch A-CNN-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0kmiXLZbMcO"
   },
   "outputs": [],
   "source": [
    "# Choose hyperparameters\n",
    "\n",
    "lr_values=[0.1,0.01,0.001,0.0001]\n",
    "w_decay_values=[1e-2,1e-3,1e-4]\n",
    "amsgrad_values=[False, True]\n",
    "\n",
    "lr_choice=3\n",
    "w_decay_choice=1\n",
    "ams_choice=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyFqqubp0sPF",
    "outputId": "3daf5019-bb26-4b04-e72a-d60b452642b3"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ACNNLSTM(emb_size,4)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer =  torch.optim.Adam(model.parameters(),lr=lr_values[lr_choice],weight_decay=w_decay_values[w_decay_choice],amsgrad=amsgrad_values[ams_choice])\n",
    "\n",
    "\n",
    "# random split to train and test\n",
    "PATH=\"model_checkpoint.pt\"\n",
    "\n",
    "\n",
    "# Split to test and validation\n",
    "train_ind, val_ind , class_lens_train, num_class = train_test_split(y_train, 0.1) \n",
    "\n",
    "\n",
    "x_cross_train=[x_train[i] for i in train_ind]\n",
    "x_cross_val=[x_train[i] for i in val_ind]\n",
    "\n",
    "y_cross_train=y_train[train_ind]\n",
    "y_cross_val=y_train[val_ind]\n",
    "\n",
    "\n",
    "\n",
    "# Imbalanced classes weighting\n",
    "\n",
    "max = np.sum(class_lens_train)/class_lens_train.shape[0]\n",
    "weight = torch.tensor(max / class_lens_train, dtype = torch.float).to(device)\n",
    " \n",
    "loss_function = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "# Get lengths of sequences\n",
    "\n",
    "len_cross_val=np.asarray([example.shape[0] for example in x_cross_val])\n",
    "\n",
    "# Pad sequences to max length\n",
    "\n",
    "seq = [torch.tensor(e) for e in x_cross_val]\n",
    "x_cross_val_padded=pad_sequence(seq,batch_first=True)\n",
    "\n",
    "# Make validation tensors since they dont shuffle\n",
    "input_val_tensor =  torch.tensor(x_cross_val_padded, dtype = torch.float).transpose(1,2).to(device) \n",
    "y_val_tensor=torch.tensor(y_cross_val, dtype = torch.long).to(device)\n",
    "\n",
    "print(input_val_tensor.shape)\n",
    "print(y_val_tensor)\n",
    "\n",
    "epochs = 600\n",
    "loss_plot=[]\n",
    "val_loss_plot=[]\n",
    "train_acc_plot=[]\n",
    "val_acc_plot=[]\n",
    "\n",
    "max_acc=0\n",
    "for i in range(epochs):\n",
    "\n",
    "      \n",
    "      model.train()\n",
    "\n",
    "      print(f\"Epoch {i+1}\")\n",
    "    \n",
    "      optimizer.zero_grad()\n",
    "   \n",
    "\n",
    "      # Shuffle train set for this epoch\n",
    "\n",
    "      c = list(zip(x_cross_train, y_cross_train))\n",
    "      random.shuffle(c)\n",
    "      x_cross_train, y_cross_train = zip(*c)\n",
    "      x_cross_train=list(x_cross_train)\n",
    "      y_cross_train=np.asarray(list(y_cross_train))\n",
    "\n",
    "     \n",
    "\n",
    "      # Getting lenght of every essay\n",
    "\n",
    "      len_cross_train=np.asarray([example.shape[0] for example in x_cross_train])\n",
    "     \n",
    "\n",
    "      # Pad sequences to max length\n",
    "      seq = [torch.tensor(e) for e in x_cross_train]\n",
    "      x_cross_train_padded=pad_sequence(seq,batch_first=True)\n",
    "\n",
    "      # Make the train tensors\n",
    "\n",
    "      inputs_tensor = torch.tensor(x_cross_train_padded, dtype = torch.float).transpose(1,2).to(device) \n",
    "\n",
    "      y_train_tensor=torch.tensor(y_cross_train, dtype = torch.long).to(device)# long\n",
    "\n",
    "\n",
    "      # Forward pass\n",
    "      y_pred = model(inputs_tensor,len_cross_train)\n",
    "\n",
    "      # Calc loss\n",
    "      single_loss = loss_function(y_pred, y_train_tensor)\n",
    "\n",
    "      \n",
    "      # Calculate gradients \n",
    "      single_loss.backward()\n",
    "\n",
    "      # Update Weights\n",
    "      optimizer.step() \n",
    "\n",
    "\n",
    "\n",
    "      loss=single_loss.item()\n",
    "      print(f\"Loss {loss}\")\n",
    "      loss_plot.append(loss)\n",
    "\n",
    "      \n",
    "\n",
    "      train_acc=np.mean((np.argmax(y_pred.cpu().detach().numpy(), axis=1)==y_cross_train)*1)\n",
    "\n",
    "      train_acc_plot.append(train_acc)\n",
    "      print(f\"Acc train {train_acc}\")\n",
    "\n",
    "      model.eval()\n",
    "\n",
    "      y_pred_val = model(input_val_tensor,len_cross_val)\n",
    "\n",
    "      # loss of validation\n",
    "\n",
    "      single_loss = loss_function(y_pred_val, y_val_tensor)\n",
    "   \n",
    "\n",
    "      val_acc=np.mean((np.argmax(y_pred_val.cpu().detach().numpy(), axis=1)==y_cross_val)*1)\n",
    "\n",
    "      val_acc_plot.append(val_acc)\n",
    "      print(f\"Acc val {val_acc}\")\n",
    "\n",
    "      val_loss=single_loss.item()\n",
    "      print(f\"Loss val {val_loss}\")\n",
    "      \n",
    "      val_loss_plot.append(val_loss)\n",
    "\n",
    "\n",
    "\n",
    "      if max_acc<val_acc: # save best weights\n",
    "          torch.save({\n",
    "                    'epoch': i,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': single_loss,\n",
    "                    }, PATH)\n",
    "          max_acc=val_acc\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PfSWJuiI0zaa",
    "outputId": "7f41f463-1fe3-4b02-943d-9c831943b92a"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_plot)\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_acc_plot)\n",
    "plt.title(\"Training accuracy\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train acc')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_loss_plot)\n",
    "plt.title(\"Validation loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('val loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_acc_plot)\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('val acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alVdlRAnV8lz"
   },
   "source": [
    "# Training full batch CNN-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_gUJcpbcpZ1"
   },
   "outputs": [],
   "source": [
    "# Choose hyperparameters\n",
    "\n",
    "lr_values=[0.1,0.01,0.001,0.0001]\n",
    "w_decay_values=[1e-2,1e-3,1e-4]\n",
    "amsgrad_values=[False, True]\n",
    "\n",
    "lr_choice=2\n",
    "w_decay_choice=1\n",
    "ams_choice=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijRwBfBDzcve",
    "outputId": "8ae0d042-3bb6-4e7e-ce21-876ca81cebc8"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNLSTM(emb_size,4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer =  torch.optim.Adam(model.parameters(),lr=lr_values[lr_choice],weight_decay=w_decay_values[w_decay_choice],amsgrad=amsgrad_values[ams_choice])\n",
    "\n",
    "\n",
    "# random split to train and test\n",
    "PATH=\"model_checkpoint.pt\"\n",
    "\n",
    "\n",
    "\n",
    "# Split to test and validation\n",
    "train_ind, val_ind , class_lens_train, num_class = train_test_split(y_train, 0.1) # can be random or cross-validation split! change later\n",
    "\n",
    "\n",
    "x_cross_train=[x_train[i] for i in train_ind]\n",
    "x_cross_val=[x_train[i] for i in val_ind]\n",
    "\n",
    "y_cross_train=y_train[train_ind]\n",
    "y_cross_val=y_train[val_ind]\n",
    "# get only smmall portion to be train and test\n",
    "\n",
    "\n",
    "\n",
    "print(y_cross_val)\n",
    "print(y_cross_train)\n",
    "\n",
    "\n",
    "# Imbalanced classes weighting\n",
    "\n",
    "max = np.sum(class_lens_train)/class_lens_train.shape[0]\n",
    "weight = torch.tensor(max / class_lens_train, dtype = torch.float).to(device)\n",
    " \n",
    "loss_function = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "# Get lengths of sequences\n",
    "\n",
    "len_cross_val=np.asarray([example.shape[0] for example in x_cross_val])\n",
    "\n",
    "# Pad sequences to max length\n",
    "\n",
    "seq = [torch.tensor(e) for e in x_cross_val]\n",
    "x_cross_val_padded=pad_sequence(seq,batch_first=True)\n",
    "\n",
    "# Make validation tensors since they dont shuffle\n",
    "input_val_tensor =  torch.tensor(x_cross_val_padded, dtype = torch.float).transpose(1,2).to(device) \n",
    "y_val_tensor=torch.tensor(y_cross_val, dtype = torch.long).to(device)# long\n",
    "\n",
    "print(input_val_tensor.shape)\n",
    "print(y_val_tensor)\n",
    "\n",
    "epochs = 800\n",
    "loss_plot=[]\n",
    "val_loss_plot=[]\n",
    "train_acc_plot=[]\n",
    "val_acc_plot=[]\n",
    "\n",
    "max_acc=0\n",
    "for i in range(epochs):\n",
    "\n",
    "      \n",
    "      model.train()\n",
    "\n",
    "      print(f\"Epoch {i+1}\")\n",
    "    \n",
    "      optimizer.zero_grad()\n",
    "   \n",
    "\n",
    "      # Shuffle train set for this epoch\n",
    "\n",
    "      c = list(zip(x_cross_train, y_cross_train))\n",
    "      random.shuffle(c)\n",
    "      x_cross_train, y_cross_train = zip(*c)\n",
    "      x_cross_train=list(x_cross_train)\n",
    "      y_cross_train=np.asarray(list(y_cross_train))\n",
    "\n",
    "     # print(y_cross_train)\n",
    "\n",
    "      # Getting lenght of every essay\n",
    "\n",
    "      len_cross_train=np.asarray([example.shape[0] for example in x_cross_train])\n",
    "     \n",
    "      # Pad sequences\n",
    "\n",
    "      # Pad sequences to max length\n",
    "      seq = [torch.tensor(e) for e in x_cross_train]\n",
    "      x_cross_train_padded=pad_sequence(seq,batch_first=True)\n",
    "\n",
    "      # Make the train tensors\n",
    "\n",
    "      inputs_tensor = torch.tensor(x_cross_train_padded, dtype = torch.float).transpose(1,2).to(device) \n",
    "\n",
    "      y_train_tensor=torch.tensor(y_cross_train, dtype = torch.long).to(device)# long\n",
    "\n",
    "\n",
    "      # Forward pass\n",
    "      y_pred = model(inputs_tensor,len_cross_train)\n",
    "\n",
    "      # Calc loss\n",
    "      single_loss = loss_function(y_pred, y_train_tensor)\n",
    "\n",
    "      \n",
    "      # Calculate gradients \n",
    "      single_loss.backward()\n",
    "\n",
    "      # Update Weights\n",
    "      optimizer.step() \n",
    "\n",
    "\n",
    "\n",
    "      loss=single_loss.item()\n",
    "      print(f\"Loss {loss}\")\n",
    "      loss_plot.append(loss)\n",
    "\n",
    "      \n",
    "\n",
    "      train_acc=np.mean((np.argmax(y_pred.cpu().detach().numpy(), axis=1)==y_cross_train)*1)\n",
    "\n",
    "      train_acc_plot.append(train_acc)\n",
    "      print(f\"Acc train {train_acc}\")\n",
    "\n",
    "      model.eval()\n",
    "\n",
    "      y_pred_val = model(input_val_tensor,len_cross_val)\n",
    "\n",
    "      # loss of validation\n",
    "\n",
    "      single_loss = loss_function(y_pred_val, y_val_tensor)\n",
    "   \n",
    "\n",
    "      val_acc=np.mean((np.argmax(y_pred_val.cpu().detach().numpy(), axis=1)==y_cross_val)*1)\n",
    "\n",
    "      val_acc_plot.append(val_acc)\n",
    "      print(f\"Acc val {val_acc}\")\n",
    "\n",
    "      val_loss=single_loss.item()\n",
    "      print(f\"Loss val {val_loss}\")\n",
    "      \n",
    "      val_loss_plot.append(val_loss)\n",
    "\n",
    "\n",
    "\n",
    "      if max_acc<val_acc: # save best weights\n",
    "          torch.save({\n",
    "                    'epoch': i,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': single_loss,\n",
    "                    }, PATH)\n",
    "          max_acc=val_acc\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8-P2Uvuz69ou",
    "outputId": "5772bb92-488b-497a-dd45-55a34cf0931b"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_plot)\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_acc_plot)\n",
    "plt.title(\"Training accuracy\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train acc')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_loss_plot)\n",
    "plt.title(\"Validation loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('val loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_acc_plot)\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('val acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MD6wFlKo1tx1",
    "outputId": "c461ca1c-05a2-4042-f31b-afe92bc47687"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNLSTM(302,True,4)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer =  torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#one_hot_labels = np.zeros((len(score_1),15))\n",
    "#one_hot_labels[np.arange(len(score_1)),score_1] = 1\n",
    "\n",
    "# random split to train and test\n",
    "PATH=\"model_checkpoint.pt\"\n",
    "\n",
    "\n",
    "# Split to test and validation\n",
    "train_ind, val_ind ,class_lens_train, num_class =train_test_split(y_train1, 0.2) # can be random or cross-validation split! change later\n",
    "\n",
    "\n",
    "x_cross_train=[x_train1[i] for i in train_ind]\n",
    "x_cross_val=[x_train1[i] for i in val_ind]\n",
    "\n",
    "y_cross_train=y_train1[train_ind]\n",
    "y_cross_val=y_train1[val_ind]\n",
    "# get only smmall portion to be train and test\n",
    "\n",
    "\n",
    "print(class_lens_train)\n",
    "# Shuffle train set\n",
    "\n",
    "#c = list(zip(x_cross_train, y_cross_train))\n",
    "#random.shuffle(c)\n",
    "#x_cross_train, y_cross_train = zip(*c)\n",
    "#x_cross_train=list(x_cross_train)\n",
    "#y_cross_train=np.asarray(list(y_cross_train))\n",
    "\n",
    "# Getting lenght of every essay\n",
    "\n",
    "len_cross_train=np.asarray([example.shape[0] for example in x_cross_train])\n",
    "len_cross_val=np.asarray([example.shape[0] for example in x_cross_val])\n",
    "print(len_cross_train.dtype)\n",
    "\n",
    "# Make the dataset\n",
    "traindata = MyDataset(x_cross_train, len_cross_train, y_cross_train)\n",
    "\n",
    "# Make the loader\n",
    "\n",
    "buckets=[0,21,31,41,61,81,101,151,301]\n",
    "buckets=[0,301]\n",
    "sampler = BySequenceLengthSampler(traindata, buckets, batch_size=1000, drop_last=False) # real batch size, 400 so it gets the whole bucket\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=1, # not real batch_size\n",
    "                        batch_sampler=sampler, \n",
    "                        num_workers=0, \n",
    "                        collate_fn=collate,\n",
    "                        drop_last=False, pin_memory=False)\n",
    "print(trainloader)\n",
    "for j in range(2):\n",
    "   for i, data in enumerate(trainloader, 0):\n",
    "      inputs, lens, labels = data\n",
    "\n",
    "      print(inputs.shape)\n",
    "      print(lens.shape)\n",
    "      print(labels.shape)\n",
    "      print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSTQ0KD064MU"
   },
   "source": [
    "# Training mini batch CNN-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8laScomMc6CG"
   },
   "outputs": [],
   "source": [
    "# Choose hyperparameters\n",
    "\n",
    "lr_values=[0.1,0.01,0.001,0.0001]\n",
    "w_decay_values=[1e-2,1e-3,1e-4]\n",
    "amsgrad_values=[False, True]\n",
    "\n",
    "lr_choice=2\n",
    "w_decay_choice=1\n",
    "ams_choice=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5AYCwbyV-dS",
    "outputId": "78a87771-4391-49b3-d20c-367b937aa321"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer =  torch.optim.Adam(model.parameters(),lr=lr_values[lr_choice],weight_decay=w_decay_values[w_decay_choice],amsgrad=amsgrad_values[ams_choice])\n",
    "\n",
    "\n",
    "# random split to train and test\n",
    "PATH=\"model_checkpoint.pt\"\n",
    "\n",
    "\n",
    "# Split to test and validation\n",
    "\n",
    "\n",
    "train_ind, val_ind ,class_lens_train, num_class=train_test_split(y_train1, 0.2) # can be random or cross-validation split! change later\n",
    "\n",
    "model = CNNLSTM(emb_size,num_class)\n",
    "model.to(device)\n",
    "\n",
    "x_cross_train=[x_train1[i] for i in train_ind]\n",
    "x_cross_val=[x_train1[i] for i in val_ind]\n",
    "\n",
    "y_cross_train=y_train1[train_ind]\n",
    "y_cross_val=y_train1[val_ind]\n",
    "\n",
    "\n",
    "# Imbalanced classes weighting\n",
    "\n",
    "max = np.sum(class_lens_train)/class_lens_train.shape[0]\n",
    "weight = torch.tensor(max / class_lens_train, dtype = torch.float).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "\n",
    "\n",
    "# Getting lenght of every essay\n",
    "\n",
    "len_cross_train=np.asarray([example.shape[0] for example in x_cross_train])\n",
    "len_cross_val=np.asarray([example.shape[0] for example in x_cross_val])\n",
    "print(len_cross_train.dtype)\n",
    "\n",
    "# Make the dataset\n",
    "traindata = MyDataset(x_cross_train, len_cross_train, y_cross_train)\n",
    "\n",
    "# Make the loader\n",
    "\n",
    "\n",
    "buckets=[0,21,41,61,81,101,151,301]\n",
    "  \n",
    "\n",
    "\n",
    "sampler = BySequenceLengthSampler(traindata, buckets, batch_size=2000, drop_last=False) # real batch size, to get whole buckets\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=1, # not real batch_size\n",
    "                        batch_sampler=sampler, \n",
    "                        num_workers=0, \n",
    "                        collate_fn=collate,\n",
    "                        drop_last=False, pin_memory=False)\n",
    "\n",
    "\n",
    "# Make the dataset for val, here we need to padd  val also in batches and predict like that\n",
    "valdata = MyDataset(x_cross_val, len_cross_val, y_cross_val)\n",
    "\n",
    "# Make the loader\n",
    "\n",
    "sampler_val = BySequenceLengthSampler(valdata, buckets, batch_size=1000, drop_last=False) # real batch size, to always put whole bucket, since its not important\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valdata, batch_size=1, # not real batch_size\n",
    "                        batch_sampler=sampler_val, \n",
    "                        num_workers=0, \n",
    "                        collate_fn=collate,\n",
    "                        drop_last=False, pin_memory=False)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "loss_plot=[]\n",
    "val_loss_plot=[]\n",
    "train_acc_plot=[]\n",
    "val_acc_plot=[]\n",
    "\n",
    "min_loss=1000\n",
    "for i in range(epochs):\n",
    "\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  print(f\"Epoch {i+1}\")\n",
    " \n",
    "  optimizer.zero_grad()\n",
    "  loss=0\n",
    "  predictions=[]\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "      inputs, lens, labels = data    #this batch has some padding but always diff \n",
    "\n",
    "      inputs_tensor=torch.tensor(inputs, dtype = torch.float).transpose(1,2).to(device) \n",
    "      print(inputs_tensor.dtype)\n",
    "      y_train_tensor=torch.tensor(labels, dtype = torch.long).to(device)# long\n",
    "      print(y_train_tensor.dtype)\n",
    "  \n",
    "    \n",
    "      # Forward pass\n",
    "      y_pred = model(inputs_tensor,lens)\n",
    "\n",
    "      # Calc loss\n",
    "      single_loss = loss_function(y_pred, y_train_tensor)\n",
    "      loss+=single_loss.item()*lens.shape[0]\n",
    "      \n",
    "      # Calculate gradients \n",
    "      single_loss.backward()\n",
    "\n",
    "      # Update Weights\n",
    "      optimizer.step() \n",
    "   \n",
    "\n",
    "      predictions.append((np.argmax(y_pred.cpu().detach().numpy(), axis=1)==labels)*1)\n",
    "  \n",
    "        \n",
    "\n",
    "\n",
    "  predictions=np.concatenate(predictions)\n",
    "  train_acc=np.mean(predictions)\n",
    "\n",
    "  train_acc_plot.append(train_acc)\n",
    "  print(f\"Acc train {train_acc}\")\n",
    "\n",
    "  \n",
    "  loss/=len_cross_train.shape[0]\n",
    "  \n",
    "  print(f\"Loss {loss}\")\n",
    "\n",
    "  loss_plot.append(loss)\n",
    "\n",
    "  model.eval()\n",
    "  # forward on validation\n",
    "\n",
    "  val_loss=0\n",
    "  predictions=[]\n",
    "  for i, data in enumerate(valloader, 0):\n",
    "      inputs, lens, labels = data    #this batch has some padded but always diff \n",
    "\n",
    "      input_val=torch.tensor(inputs, dtype = torch.float).transpose(1,2).to(device) \n",
    "    \n",
    "      y_val_tensor=torch.tensor(labels, dtype = torch.long).to(device)# long for class\n",
    "  \n",
    "    \n",
    "      y_pred_val = model(input_val,lens)\n",
    "\n",
    "      # loss of validation of one batch\n",
    "\n",
    "      single_loss = loss_function(y_pred_val, y_val_tensor)\n",
    " \n",
    "      val_loss+=single_loss.item()*lens.shape[0]\n",
    "      if classification:\n",
    "\n",
    "          predictions.append((np.argmax(y_pred_val.cpu().detach().numpy(), axis=1)==labels)*1)\n",
    "      else:\n",
    "          predictions.append((np.round(y_pred_val.cpu().detach().numpy())==labels)*1)\n",
    "\n",
    "  predictions=np.concatenate(predictions)\n",
    "  val_acc=np.mean(predictions)\n",
    "\n",
    "  val_acc_plot.append(val_acc)\n",
    "  print(f\"Acc val {val_acc}\")\n",
    "\n",
    "\n",
    "  val_loss/=len_cross_val.shape[0]\n",
    "\n",
    "  print(f\"Loss val {val_loss}\")\n",
    "\n",
    "  val_loss_plot.append(val_loss)\n",
    "\n",
    "  if min_loss>val_loss: # save best weights\n",
    "      torch.save({\n",
    "                'epoch': i,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': single_loss,\n",
    "                }, PATH)\n",
    "      min_loss=val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0eD4ipAxK-yC",
    "outputId": "118e21e4-196d-4991-94f3-8fe67cf05b65"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_plot)\n",
    "plt.title(\"Train loss\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_acc_plot)\n",
    "plt.title(\"Train acc\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_loss_plot)\n",
    "plt.title(\"Val loss\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_acc_plot)\n",
    "plt.title(\"Val acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOw8IUL37n_Z"
   },
   "source": [
    "# Predicting mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "PydExWroWEhe",
    "outputId": "4ef7a28d-2014-4714-ec63-af8a953cf8b0"
   },
   "outputs": [],
   "source": [
    "# Predict with best weights on train set\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "predictions=[]\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "      inputs, lens, labels = data    #this batch has some padded but always diff \n",
    "\n",
    "      inputs_tensor=torch.tensor(inputs, dtype = torch.float).transpose(1,2).to(device) \n",
    "    \n",
    "      y_train_tensor=torch.tensor(labels, dtype = torch.long).to(device)\n",
    "  \n",
    "    \n",
    "      # Forward pass\n",
    "      y_pred = model(inputs_tensor,lens)\n",
    "      \n",
    "      predictions.append((np.argmax(y_pred.cpu().detach().numpy(), axis=1)==labels)*1)\n",
    "\n",
    "predictions=np.concatenate(predictions)\n",
    "print(predictions)\n",
    "print(y_cross_train)\n",
    "\n",
    "accuracy_1 = np.mean(predictions) # if the score is exact\n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhivxYq6WITZ",
    "outputId": "12927e3b-f27a-412f-b905-8cf46c35a75c"
   },
   "outputs": [],
   "source": [
    "# Predict with best weights on validation set\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions_val=[]\n",
    "for i, data in enumerate(valloader, 0):\n",
    "      inputs, lens, labels = data    #this batch has some padded but always diff \n",
    "\n",
    "      inputs_tensor=torch.tensor(inputs, dtype = torch.float).transpose(1,2).to(device) \n",
    "    \n",
    "      y_train_tensor=torch.tensor(labels, dtype = torch.long).to(device)\n",
    "  \n",
    "    \n",
    "      # Forward pass\n",
    "      y_pred = model(inputs_tensor,lens)\n",
    "      predictions_val.append((np.argmax(y_pred.cpu().detach().numpy(), axis=1)==labels)*1)\n",
    "\n",
    "\n",
    "\n",
    "predictions_val=np.concatenate(predictions_val)\n",
    "\n",
    "accuracy_1 = np.mean(predictions_val)\n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZHI_rB9aYk4"
   },
   "source": [
    "# Predicting full batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYwuBZcxaWtP",
    "outputId": "4f458b1e-e1d7-4b54-d714-1f0219566bde"
   },
   "outputs": [],
   "source": [
    "# Predict with best weights on train set\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "    \n",
    "# Forward pass\n",
    "y_pred = model(inputs_tensor,len_cross_train)\n",
    "\n",
    "predictions=np.argmax(y_pred.cpu().detach().numpy(), axis=1)\n",
    "\n",
    "accuracy_1=np.mean((predictions==y_cross_train)*1)# if the score is exact \n",
    "\n",
    "\n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7hCtijMGQZ5",
    "outputId": "7e9c0f88-2d42-4b56-b153-0815ed43c869"
   },
   "outputs": [],
   "source": [
    "# Predict with best weights on validation set\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "    \n",
    "# Forward pass\n",
    "y_pred = model(input_val_tensor,len_cross_val)\n",
    "\n",
    "predictions_val=np.argmax(y_pred.cpu().detach().numpy(), axis=1)\n",
    "\n",
    "accuracy_1=np.mean((predictions_val==y_cross_val)*1)# if the score is exact \n",
    "\n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RWX2cYGf6N8"
   },
   "source": [
    "# Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N2rny3Jd3_k"
   },
   "outputs": [],
   "source": [
    "# Kappa score\n",
    "\n",
    "import numpy as np\n",
    "from kappa import quadratic_weighted_kappa as qwk\n",
    "from kappa import linear_weighted_kappa as lwk\n",
    "\n",
    "def assert_inputs(rater_a, rater_b):\n",
    "\tassert np.issubdtype(rater_a.dtype, np.integer), 'Integer array expected, got ' + str(rater_a.dtype)\n",
    "\tassert np.issubdtype(rater_b.dtype, np.integer), 'Integer array expected, got ' + str(rater_b.dtype)\n",
    "\n",
    "def quadratic_weighted_kappa(rater_a, rater_b, min_rating, max_rating):\n",
    "\tassert_inputs(rater_a, rater_b)\n",
    "\treturn qwk(rater_a, rater_b, min_rating, max_rating)\n",
    "\n",
    "def linear_weighted_kappa(rater_a, rater_b, min_rating, max_rating):\n",
    "\tassert_inputs(rater_a, rater_b)\n",
    "\treturn lwk(rater_a, rater_b, min_rating, max_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RL3qgfzsn4V5",
    "outputId": "4bd453dc-3a66-4dde-cf46-c20ecb8cf0fd"
   },
   "outputs": [],
   "source": [
    "# Kappa score for train and validaiton set\n",
    "print(quadratic_weighted_kappa(predictions.astype('int'), y_cross_train.astype('int'),min_rating=0, max_rating=3))\n",
    "print(quadratic_weighted_kappa(predictions_val.astype('int'), y_cross_val.astype('int'),min_rating=0, max_rating=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvBbDP-bUP2b"
   },
   "source": [
    "# Predicting for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWzascSSUSMi",
    "outputId": "06dcf0a7-00a7-42e6-8448-e899c5484a16"
   },
   "outputs": [],
   "source": [
    " #Get lengths of sequences\n",
    "\n",
    "len_test=np.asarray([example.shape[0] for example in x_test])\n",
    "\n",
    "# Pad sequences to max length\n",
    "\n",
    "seq = [torch.tensor(e) for e in x_test]\n",
    "x_test_padded=pad_sequence(seq,batch_first=True)\n",
    "\n",
    "# Make validation tensors since they dont shuffle\n",
    "input_test_tensor =  torch.tensor(x_test_padded, dtype = torch.float).transpose(1,2).to(device) #\n",
    "y_test_tensor=torch.tensor(y_test, dtype = torch.long).to(device)# long\n",
    "\n",
    "# Predict with best weights\n",
    "PATH=\"model_checkpoint.pt\"\n",
    "model = ACNNLSTM(emb_size,4)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "print(epoch)\n",
    "model.eval()\n",
    "\n",
    "    \n",
    "# Forward pass\n",
    "y_pred = model(input_test_tensor,len_test)\n",
    "\n",
    "\n",
    "predictions=np.argmax(y_pred.cpu().detach().numpy(), axis=1)\n",
    "\n",
    "accuracy=np.mean((predictions==y_test)*1)# if the score is exact \n",
    "print(accuracy)\n",
    "\n",
    "# Kappa\n",
    "\n",
    "print(quadratic_weighted_kappa(predictions.astype('int'), y_test.astype('int'),min_rating=0, max_rating=3))\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# baseline trait 2 is 0.5813467168548725"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lUusUOnYJOES"
   ],
   "name": "NLP_CNNLSTM_final_version.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
